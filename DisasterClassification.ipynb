{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b270c01b-23db-49b2-8481-22fa2fd211fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65412f9c-e4d3-4505-ad4b-3a408208976c",
   "metadata": {},
   "source": [
    "<p style=\"font-size:18px;\">\n",
    "    To utilise the pre-written code and functionality the libraries given below are imported. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483f6c83-2840-46e1-be45-3d7bf6ead3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/savinaysingh/Documents/SavinayUTS/iLab/Code/iLab-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import inflect\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, losses\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00fef2-f8ab-4563-b0be-7cc094701185",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd10a3d-4928-4f66-89ab-df6c62c3b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/savinaysingh/Downloads/tweets 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b22115a-1a1a-48ed-a19a-977cb480ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113e36b-a370-4754-ba7f-04cbd3c0454e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8826456f-4166-4fd4-90e4-d3c56de38b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the words having apostophe into their root form\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"there\\'s\", \"there is\", phrase)\n",
    "    phrase = re.sub(r\"it\\'s\", \"it is\", phrase)\n",
    "    phrase = re.sub(r\"he\\'s\", \"he is\", phrase)\n",
    "    phrase = re.sub(r\"she\\'s\", \"she is\", phrase)\n",
    "    phrase = re.sub(r\"how\\'s\", \"how is\", phrase)\n",
    "    phrase = re.sub(r\"let\\'s\", \"let is\", phrase)\n",
    "    phrase = re.sub(r\"so\\'s\", \"so is\", phrase)\n",
    "    phrase = re.sub(r\"what\\'s\", \"what is\", phrase)\n",
    "    phrase = re.sub(r\"when\\'s\", \"when is\", phrase)\n",
    "    phrase = re.sub(r\"where\\'s\", \"where is\", phrase)\n",
    "    phrase = re.sub(r\"why\\'s\", \"why is\", phrase)\n",
    "  \n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06741ae4-8bf7-4ecf-8f12-955e7d3a9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-emojize the text\n",
    "def remove_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# url not req while finding if text is disaster or not\n",
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    pattern = re.compile(r'#\\w+')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# spl chars not req while finding if text is disaster or not\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be61e584-5786-4418-8225-be528f3d4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_emojis(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b178ad-13a4-479f-8acf-beaade1813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDash(input_string):\n",
    "        return re.sub(r'-', ' ', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d1c071-995f-4fac-9d90-9dbd54607dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(preprocessed_text):\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        # print(tag)\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "   # 2. Lemmatize a Sentence with the appropriate POS tag\n",
    "    lemmatized_text=[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(preprocessed_text)]\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    text_final=(\" \".join( lemmatized_text ))\n",
    "    return text_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234d4961-7204-4fa5-baee-3da527d46ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f629e10-37a0-4d39-b4fe-bddd2c090abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame containing text data by applying various cleaning and text processing steps.\n",
    "    \n",
    "    Args:\n",
    "    data (DataFrame): The input DataFrame containing text data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The preprocessed DataFrame with cleaned text data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"1. Capturing Hashtags\")\n",
    "    # 1. Capturing Hashtags\n",
    "    pattern = r'#\\w+'\n",
    "    data['hashtags'] = data['text'].str.findall(pattern)\n",
    "    data.hashtags=['' if [] == d else ' '.join([remove_special_characters(w) for w in d]) for d in data.hashtags]\n",
    "    # 2. Handling Apostrophes\n",
    "    print(\"2. Handling Apostrophes\")\n",
    "    sentences = data['text']\n",
    "    number_sent_apostophe=len([x for x in sentences if \"'\" in x])\n",
    "    print(\"Percentage of sentences with apostophe is (before handling): \",number_sent_apostophe/len(sentences)*100,\"%\")\n",
    "    sentences = [decontracted(x) for x in sentences]\n",
    "    data['text'] = sentences\n",
    "    number_sent_apostophe=len([x for x in sentences if \"'\" in x])\n",
    "    print(\"Percentage of sentences with apostophe is (after handling): \",number_sent_apostophe/len(sentences)*100,\"%\")\n",
    "\n",
    "    # 3. Lowercasing\n",
    "    print(\"3. Lowercasing the sentences\")\n",
    "    data['text'] = [entry.lower() for entry in sentences]\n",
    "\n",
    "    # 4. Removing Emojis, URLs, Hashtags, and Special Characters\n",
    "    print(\"4. Removing Emojis, URLs, Hashtags, and Special Characters\")\n",
    "    data['text'] = data['text'].apply(preprocess_text)\n",
    "    \n",
    "    \n",
    "    # 5. Handling Numerics\n",
    "    print(\"5. Handling Numerics\")\n",
    "    p = inflect.engine()\n",
    "    sentences = list(data.text)\n",
    "    data.text = sentences\n",
    "    final_ans = []\n",
    "    for i, sent in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "        list_numbers = re.findall(r'[0-9]+', sent)\n",
    "        list_numbers = [int(l) for l in list_numbers]\n",
    "        list_words = [p.number_to_words(l) for l in list_numbers]\n",
    "        dict_NW = dict(zip(list_numbers, list_words))\n",
    "        myKeys = list(dict_NW.keys())\n",
    "        myKeys.sort(reverse=True)\n",
    "        sorted_dict = {i: dict_NW[i] for i in myKeys}\n",
    "        ans = copy.copy(sent)\n",
    "        for key in sorted_dict.keys():\n",
    "            ans = re.sub(str(key), ' ' + sorted_dict[key] + ' ', ans)\n",
    "        final_ans.append(ans)\n",
    "    sentences = final_ans.copy()\n",
    "    data.text = sentences\n",
    "    data['text'] = data['text'].apply(removeDash)\n",
    "\n",
    "    # 6. Removing Duplicate Tweets\n",
    "    print(\"6. Removing Duplicate Tweets\")\n",
    "    data = data.drop_duplicates(subset=['text'])\n",
    "    sentences=data.text\n",
    "    # 7. Lemmatization\n",
    "    print(\"7. Performing Lemmatization\")\n",
    "    ans = []\n",
    "    for x in tqdm(sentences, total=len(sentences)):\n",
    "        ans.append(lemmatize_text(x))\n",
    "    sentences = ans.copy()\n",
    "    data.text = sentences\n",
    "\n",
    "    # 8. Stopwords Removal\n",
    "    print(\"8. Removing Stopwords\")\n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences = [' '.join([word for word in x.split() if word not in stop_words]) for x in sentences]\n",
    "    data.text = sentences\n",
    "    \n",
    "    print(\"Done\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b9e342-748c-4e6f-abbd-a38ef6b77b26",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Capturing Hashtags\n",
      "2. Handling Apostrophes\n",
      "Percentage of sentences with apostophe is (before handling):  16.710642040457344 %\n",
      "Percentage of sentences with apostophe is (after handling):  8.988566402814424 %\n",
      "3. Lowercasing the sentences\n",
      "4. Removing Emojis, URLs, Hashtags, and Special Characters\n",
      "5. Handling Numerics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 11370/11370 [00:00<00:00, 82372.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Removing Duplicate Tweets\n",
      "7. Performing Lemmatization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10938/10938 [00:18<00:00, 599.72it/s]\n",
      "/var/folders/0h/j84rsdwd49jbw1_d04_c5b3r0000gn/T/ipykernel_22084/1658894723.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.text = sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Removing Stopwords\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/j84rsdwd49jbw1_d04_c5b3r0000gn/T/ipykernel_22084/1658894723.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.text = sentences\n"
     ]
    }
   ],
   "source": [
    "data=preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f14843-4c3d-41e6-94a3-4168c03bca25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead23ec1-9b94-4c37-8efb-cc3d15ca0fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['text','hashtags']], data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e413a-4af1-4aff-8400-bc26842edb6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bdfa4-b05f-4ee7-9b1c-983530d5e34d",
   "metadata": {},
   "source": [
    "### >Vectorization for Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a4c109-8d0c-4079-ab4e-a50a6e074c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e698c9e7-1bdf-4005-bf67-37abec40c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = vectorizer.fit_transform(X_train['text'])\n",
    "countv_train=pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "matrix2 = vectorizer2.fit_transform(X_train['hashtags'])\n",
    "countv_train2=pd.DataFrame(matrix2.toarray(),columns=vectorizer2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4688515c-0687-499e-aafe-7f12a0be8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "countv_train_3=pd.concat([countv_train, countv_train2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf8625db-0ab1-4360-92b1-7d0dc2e476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Set\n",
    "matrix = vectorizer.transform(X_test['text'])\n",
    "countv_test=pd.DataFrame(matrix.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "matrix2 = vectorizer2.transform(X_test['hashtags'])\n",
    "countv_test2=pd.DataFrame(matrix2.toarray(),columns=vectorizer2.get_feature_names_out())\n",
    "countv_test_3=pd.concat([countv_test, countv_test2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06ceb7f-b99d-4110-8c36-06b7d1bd0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TF=countv_train_3\n",
    "X_test_TF=countv_test_3\n",
    "y_train_TF, y_test_TF=y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8517676-6b70-4d72-a8e8-f9ba8b7ced1f",
   "metadata": {},
   "source": [
    "### >Vectorization for Contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4a4110-b31b-47b7-b1bd-36f53fe48752",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a26d62f1-7897-4110-acf4-33558245e145",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_ST=model.encode(list(X_train.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1c3bfd9-aed6-4f3a-a539-895306f39cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ST=model.encode(list(X_test.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a16bcddc-4b61-4295-aacb-5195a3dddd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ST_x=np.concatenate((X_train_ST, countv_train2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26bbdcab-2a68-4817-82c0-ab7f0834eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ST_x=np.concatenate((X_test_ST, countv_test2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f98183-9204-44fd-ab9a-5921baddee93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4166178b-6eda-4c83-80a7-d8b7812fa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ST_x_sm, y_train_ST_x_sm = smote.fit_resample(X_train_ST_x, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f400129-94da-47d1-b7d2-6fb90161f633",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e31b0e8f-9816-4c99-948f-68eb04fa0eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_classifier=RandomForestClassifier(n_estimators=100, random_state=42,class_weight={0:5,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9fa04e63-2579-4ae3-9bae-882309bd1cd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 5, 1: 1}, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 5, 1: 1}, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 5, 1: 1}, random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(X_train_ST_x_sm, y_train_ST_x_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f5654c5-dad6-422a-8176-b5d1033ecc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rf_classifier.predict(X_test_ST_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9ee8c30-6613-4f1e-88f9-8a5ae2f8f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1794\n",
      "           1       0.63      0.73      0.68       398\n",
      "\n",
      "    accuracy                           0.87      2192\n",
      "   macro avg       0.79      0.82      0.80      2192\n",
      "weighted avg       0.88      0.87      0.88      2192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8886541-7422-42c5-8cb8-5fc602f53327",
   "metadata": {},
   "source": [
    "## 8. Filtering out tweets in Harvey Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8725a5c-d81d-4a6c-bb3c-aec3b683848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harvey=pd.read_csv('/Users/savinaysingh/Downloads/Hurricane_Harvey.csv', header= 0,\n",
    "                        encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c05b59ca-888a-4bd7-b534-f6d55640e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harvey['text']=df_harvey['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d773a7d0-93cd-4533-9793-38e34fae3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harvey=df_harvey[~df_harvey['Tweet'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a69af69-86f5-48eb-92cc-2b7657ecfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Capturing Hashtags\n",
      "2. Handling Apostrophes\n",
      "Percentage of sentences with apostophe is (before handling):  17.777928185382386 %\n",
      "Percentage of sentences with apostophe is (after handling):  10.42575379277843 %\n",
      "3. Lowercasing the sentences\n",
      "4. Removing Emojis, URLs, Hashtags, and Special Characters\n",
      "5. Handling Numerics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 398916/398916 [00:05<00:00, 76203.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Removing Duplicate Tweets\n",
      "7. Performing Lemmatization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 258806/258806 [05:50<00:00, 738.16it/s]\n",
      "/var/folders/0h/j84rsdwd49jbw1_d04_c5b3r0000gn/T/ipykernel_22084/1658894723.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.text = sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Removing Stopwords\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/j84rsdwd49jbw1_d04_c5b3r0000gn/T/ipykernel_22084/1658894723.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.text = sentences\n"
     ]
    }
   ],
   "source": [
    "df_harvey=preprocess_data(df_harvey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4e22288-9b7a-4838-b2f5-184bb296d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding=model.encode(list(df_harvey[0:50000].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9091499d-20fc-45f4-8bc1-ebe2bc774abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8e11b1a-aa85-4d7f-a00e-9e5a3167bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2 =  vectorizer2.transform(df_harvey[0:50000]['hashtags'])\n",
    "countv_test2=pd.DataFrame(matrix2.toarray(),columns=vectorizer2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "886913d8-6291-4adc-ac94-d5121ec7d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2427)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countv_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e53a8af-99ad-44af-838d-4fc0df989640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X=np.concatenate((encoding, countv_test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b11c960-6648-4354-88df-e840ca53c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rf_classifier.predict(df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e8b564f-78f4-40c6-b0fa-84895971022c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29678\n",
       "1    20322\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f60accac-fcc3-48f7-85d6-3eae601885a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harvey=df_harvey[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7186bef3-ee5d-4f7c-84cb-3bba621d0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0h/j84rsdwd49jbw1_d04_c5b3r0000gn/T/ipykernel_22084/2164798147.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_harvey['IsDisaster']=y_pred\n"
     ]
    }
   ],
   "source": [
    "df_harvey['IsDisaster']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c68d90df-1d65-4d00-9b0f-d45be82edf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harvey.to_csv('/Users/savinaysingh/Downloads/harvey_emotion_final_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iLab-env",
   "language": "python",
   "name": "ilab-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
